{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dagmaros27/AIMS_Notebooks/blob/main/CUDA_Practical_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CUDA Programming on NVIDIA GPUs**\n",
        "\n",
        "# **Practical 4**\n",
        "\n",
        "Again make sure the correct Runtime is being used, by clicking on the Runtime option at the top, then \"Change runtime type\", and selecting an appropriate GPU such as the T4.\n",
        "\n",
        "Then verify the details of the GPU which is available to you, and upload the usual two header files."
      ],
      "metadata": {
        "id": "i1JlUA_e44zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uboEpcMD4xYA",
        "outputId": "598b005a-7a5a-497f-ccb0-b9100600b470"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan 28 15:41:40 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_cuda.h\n",
        "!wget https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_string.h\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv1nyjTmTmr7",
        "outputId": "97d81c38-e1fa-47f8-ec00-11c0a4bf98be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-28 15:41:44--  https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_cuda.h\n",
            "Resolving people.maths.ox.ac.uk (people.maths.ox.ac.uk)... 129.67.184.129, 2001:630:441:201::8143:b881\n",
            "Connecting to people.maths.ox.ac.uk (people.maths.ox.ac.uk)|129.67.184.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27832 (27K) [text/x-chdr]\n",
            "Saving to: ‘helper_cuda.h’\n",
            "\n",
            "helper_cuda.h       100%[===================>]  27.18K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2026-01-28 15:41:45 (199 KB/s) - ‘helper_cuda.h’ saved [27832/27832]\n",
            "\n",
            "--2026-01-28 15:41:45--  https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_string.h\n",
            "Resolving people.maths.ox.ac.uk (people.maths.ox.ac.uk)... 129.67.184.129, 2001:630:441:201::8143:b881\n",
            "Connecting to people.maths.ox.ac.uk (people.maths.ox.ac.uk)|129.67.184.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14875 (15K) [text/x-chdr]\n",
            "Saving to: ‘helper_string.h’\n",
            "\n",
            "helper_string.h     100%[===================>]  14.53K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2026-01-28 15:41:45 (363 KB/s) - ‘helper_string.h’ saved [14875/14875]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "The next step is to create the file reduction.cu which includes within it a reference C++ routine against which the CUDA results are compared."
      ],
      "metadata": {
        "id": "RD6IjBwY2Ltm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reduction.cu\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "//\n",
        "// Practical 4 -- initial code for shared memory reduction for\n",
        "//                a single block which is a power of two in size\n",
        "//\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "#include <float.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "float reduction_gold(float* idata, int len)\n",
        "{\n",
        "  float sum = 0.0f;\n",
        "  for(int i=0; i<len; i++) sum += idata[i];\n",
        "\n",
        "  return sum;\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void reduction(float *g_odata, float *g_idata)\n",
        "{\n",
        "    // dynamically allocated shared memory\n",
        "\n",
        "    extern  __shared__  float temp[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    // first, each thread loads data into shared memory\n",
        "\n",
        "    temp[tid] = g_idata[tid];\n",
        "\n",
        "    // next, we perform binary tree reduction\n",
        "\n",
        "    for (int d=blockDim.x/2; d>0; d=d/2) {\n",
        "      __syncthreads();  // ensure previous step completed\n",
        "      if (tid<d)  temp[tid] += temp[tid+d];\n",
        "    }\n",
        "\n",
        "    // finally, first thread puts result into global memory\n",
        "\n",
        "    if (tid==0) g_odata[0] = temp[0];\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Program main\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main( int argc, const char** argv)\n",
        "{\n",
        "  int num_blocks, num_threads, num_elements, mem_size, shared_mem_size;\n",
        "\n",
        "  float *h_data, *d_idata, *d_odata;\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  num_blocks   = 1;  // start with only 1 thread block\n",
        "  num_threads  = 512;\n",
        "  num_elements = num_blocks*num_threads;\n",
        "  mem_size     = sizeof(float) * num_elements;\n",
        "\n",
        "  // allocate host memory to store the input data\n",
        "  // and initialize to integer values between 0 and 10\n",
        "\n",
        "  h_data = (float*) malloc(mem_size);\n",
        "\n",
        "  for(int i = 0; i < num_elements; i++)\n",
        "    h_data[i] = floorf(10.0f*(rand()/(float)RAND_MAX));\n",
        "\n",
        "  // compute reference solution\n",
        "\n",
        "  float sum = reduction_gold(h_data, num_elements);\n",
        "\n",
        "  // allocate device memory input and output arrays\n",
        "\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_idata, mem_size) );\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_odata, sizeof(float)) );\n",
        "\n",
        "  // copy host memory to device input array\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(d_idata, h_data, mem_size,\n",
        "                              cudaMemcpyHostToDevice) );\n",
        "\n",
        "  // execute the kernel\n",
        "\n",
        "  shared_mem_size = sizeof(float) * num_threads;\n",
        "  reduction<<<num_blocks,num_threads,shared_mem_size>>>(d_odata,d_idata);\n",
        "  getLastCudaError(\"reduction kernel execution failed\");\n",
        "\n",
        "  // copy result from device to host\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(h_data, d_odata, sizeof(float),\n",
        "                              cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // check results\n",
        "\n",
        "  printf(\"reduction error = %f\\n\",h_data[0]-sum);\n",
        "\n",
        "  // cleanup memory\n",
        "\n",
        "  free(h_data);\n",
        "  checkCudaErrors( cudaFree(d_idata) );\n",
        "  checkCudaErrors( cudaFree(d_odata) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcwQANS22i3Q",
        "outputId": "98d12bd2-6a4f-48bb-b073-5cd76b249ffe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reduction.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "We can now compile and run the executable.\n"
      ],
      "metadata": {
        "id": "yds03ug532rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc reduction.cu -o reduction -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFHWm4Dd3_hw",
        "outputId": "9a89103b-595f-4383-b849-31a0c72df56c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z9reductionPfS_' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z9reductionPfS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 10 registers, 368 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7jX9dSAaLj0",
        "outputId": "e8b523ee-830e-4f3f-d4ad-c0a51fed63e3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "reduction error = 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4\n",
        "Compile and run the executable reduction, and check that it gets the correct\n",
        "result. Put the output in your notebook and explain why this shows the result\n",
        "is correct, and how the code has performed the required check."
      ],
      "metadata": {
        "id": "VZ1XB3HCQY6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "The output shows a reduction error of 0.0, which indicates that the GPU kernel has produced the correct result. This check is performed by comparing the value computed on the GPU with the reference result computed on the CPU using `reduction_gold`. Since the program prints the difference between these two values and the difference is zero, it confirms that the shared-memory reduction kernel correctly sums all input elements."
      ],
      "metadata": {
        "id": "2Jl7GdEh6N2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "By going back to the previous code block you can modify the code to complete the Practical 4 exercises. Remember to first make your own copy of the notebook so that you are able to edit it.\n",
        "\n",
        "For the first exercise, it may be useful to know that the following line of code will round up the input n to the nearest power of 2, so then dividing it by 2 gives the largest power of 2 less than n.\n",
        "\n",
        "`for (m=1; m<n; m=2*m) {} `\n",
        "\n",
        "For students doing this as an assignment to be assessed, you should again add your name to the title of the notebook (as in \"Practical 4 -- Mike Giles.ipynb\"), make it shared (see the Share option in the top-right corner) and provide the shared link as the submission mechanism.\n",
        "\n"
      ],
      "metadata": {
        "id": "ncymVLmd4L82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5\n",
        "\n",
        "The code currently assumes the number of threads is a power of 2.\n",
        "Extend it to handle the general case by finding the largest power of 2 less than\n",
        "blockSize, and adding the elements beyond that point to the corresponding\n",
        "first set of elements of that size. Test it with 192 threads."
      ],
      "metadata": {
        "id": "olXEW_Ym6K16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reduction.cu\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "//\n",
        "// Practical 4 -- initial code for shared memory reduction for\n",
        "//                a single block which is a power of two in size\n",
        "//\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "#include <float.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "float reduction_gold(float* idata, int len)\n",
        "{\n",
        "  float sum = 0.0f;\n",
        "  for(int i=0; i<len; i++) sum += idata[i];\n",
        "\n",
        "  return sum;\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void reduction(float *g_odata, float *g_idata)\n",
        "{\n",
        "    // dynamically allocated shared memory\n",
        "\n",
        "    extern  __shared__  float temp[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    // first, each thread loads data into shared memory\n",
        "\n",
        "    temp[tid] = g_idata[tid];\n",
        "\n",
        "    int m;\n",
        "    for (m=1; m<blockDim.x; m=2*m) {}\n",
        "    m = m/2;\n",
        "\n",
        "    if (tid + m < blockDim.x) {\n",
        "        temp[tid] += temp[tid + m];\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // next, we perform binary tree reduction\n",
        "\n",
        "    for (int d=m/2; d>0; d=d/2) {\n",
        "      __syncthreads();  // ensure previous step completed\n",
        "      if (tid<d)  temp[tid] += temp[tid+d];\n",
        "    }\n",
        "\n",
        "    // finally, first thread puts result into global memory\n",
        "\n",
        "    if (tid==0) g_odata[0] = temp[0];\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Program main\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main( int argc, const char** argv)\n",
        "{\n",
        "  int num_blocks, num_threads, num_elements, mem_size, shared_mem_size;\n",
        "\n",
        "  float *h_data, *d_idata, *d_odata;\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  num_blocks   = 1;  // start with only 1 thread block\n",
        "  num_threads  = 192;\n",
        "  num_elements = num_blocks*num_threads;\n",
        "  mem_size     = sizeof(float) * num_elements;\n",
        "\n",
        "  // allocate host memory to store the input data\n",
        "  // and initialize to integer values between 0 and 10\n",
        "\n",
        "  h_data = (float*) malloc(mem_size);\n",
        "\n",
        "  for(int i = 0; i < num_elements; i++)\n",
        "    h_data[i] = floorf(10.0f*(rand()/(float)RAND_MAX));\n",
        "\n",
        "  // compute reference solution\n",
        "\n",
        "  float sum = reduction_gold(h_data, num_elements);\n",
        "\n",
        "  // allocate device memory input and output arrays\n",
        "\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_idata, mem_size) );\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_odata, sizeof(float)) );\n",
        "\n",
        "  // copy host memory to device input array\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(d_idata, h_data, mem_size,\n",
        "                              cudaMemcpyHostToDevice) );\n",
        "\n",
        "  // execute the kernel\n",
        "\n",
        "  shared_mem_size = sizeof(float) * num_threads;\n",
        "  reduction<<<num_blocks,num_threads,shared_mem_size>>>(d_odata,d_idata);\n",
        "  getLastCudaError(\"reduction kernel execution failed\");\n",
        "\n",
        "  // copy result from device to host\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(h_data, d_odata, sizeof(float),\n",
        "                              cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // check results\n",
        "\n",
        "  printf(\"reduction error = %f\\n\",h_data[0]-sum);\n",
        "\n",
        "  // cleanup memory\n",
        "\n",
        "  free(h_data);\n",
        "  checkCudaErrors( cudaFree(d_idata) );\n",
        "  checkCudaErrors( cudaFree(d_odata) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "}\n"
      ],
      "metadata": {
        "id": "UYoDtXgxzdHT",
        "outputId": "fbe933af-5826-4633-d53f-d8ed62030813",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reduction.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc reduction.cu -o reduction -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e218f8d9-4bdd-4319-d3f9-268873400730",
        "id": "DcR2CoaFQHP7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z9reductionPfS_' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z9reductionPfS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 10 registers, 368 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5292854-3d2c-40a9-8d35-9dd32bbd7f95",
        "id": "ARiNu3t9QHQB"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "reduction error = 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6\n",
        "\n",
        "The code currently performs the reduction operation for a single thread block.\n",
        "Modify the code to perform reduction using 1000 blocks each with 512\n",
        "threads, with each block working with a different section of an input array of\n",
        "size 512000."
      ],
      "metadata": {
        "id": "gkHVXqZ5EWkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reduction.cu\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "//\n",
        "// Practical 4 -- initial code for shared memory reduction for\n",
        "//                a single block which is a power of two in size\n",
        "//\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "#include <float.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "float reduction_gold(float* idata, int len)\n",
        "{\n",
        "  float sum = 0.0f;\n",
        "  for(int i=0; i<len; i++) sum += idata[i];\n",
        "\n",
        "  return sum;\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void reduction(float *g_odata, float *g_idata)\n",
        "{\n",
        "    // dynamically allocated shared memory\n",
        "\n",
        "    extern  __shared__  float temp[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    // first, each thread loads data into shared memory\n",
        "\n",
        "    temp[tid] = g_idata[tid + blockDim.x * blockIdx.x];\n",
        "\n",
        "    int m;\n",
        "    for (m=1; m<blockDim.x; m=2*m) {}\n",
        "    m = m/2;\n",
        "\n",
        "    if (tid + m < blockDim.x) {\n",
        "        temp[tid] += temp[tid + m];\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // next, we perform binary tree reduction\n",
        "\n",
        "    for (int d=m/2; d>0; d=d/2) {\n",
        "      __syncthreads();  // ensure previous step completed\n",
        "      if (tid<d)  temp[tid] += temp[tid+d];\n",
        "    }\n",
        "\n",
        "    // finally, first thread puts result into global memory\n",
        "\n",
        "    if (tid==0) atomicAdd(g_odata,temp[0]);\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Program main\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main( int argc, const char** argv)\n",
        "{\n",
        "  int num_blocks, num_threads, num_elements, mem_size, shared_mem_size;\n",
        "\n",
        "  float *h_data, *d_idata, *d_odata;\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  num_blocks   = 1000;  // start with only 1 thread block\n",
        "  num_threads  = 512;\n",
        "  num_elements = num_blocks*num_threads;\n",
        "  mem_size     = sizeof(float) * num_elements;\n",
        "\n",
        "  // allocate host memory to store the input data\n",
        "  // and initialize to integer values between 0 and 10\n",
        "\n",
        "  h_data = (float*) malloc(mem_size);\n",
        "\n",
        "  for(int i = 0; i < num_elements; i++)\n",
        "    h_data[i] = floorf(10.0f*(rand()/(float)RAND_MAX));\n",
        "\n",
        "  // compute reference solution\n",
        "\n",
        "  float sum = reduction_gold(h_data, num_elements);\n",
        "\n",
        "  // allocate device memory input and output arrays\n",
        "\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_idata, mem_size) );\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_odata, sizeof(float)) );\n",
        "\n",
        "  // copy host memory to device input array\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(d_idata, h_data, mem_size,\n",
        "                              cudaMemcpyHostToDevice) );\n",
        "\n",
        "  // execute the kernel\n",
        "\n",
        "  shared_mem_size = sizeof(float) * num_threads;\n",
        "  reduction<<<num_blocks,num_threads,shared_mem_size>>>(d_odata,d_idata);\n",
        "  getLastCudaError(\"reduction kernel execution failed\");\n",
        "\n",
        "  // copy result from device to host\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(h_data, d_odata, sizeof(float),\n",
        "                              cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // check results\n",
        "\n",
        "  printf(\"reduction error = %f\\n\",h_data[0]-sum);\n",
        "\n",
        "  // cleanup memory\n",
        "\n",
        "  free(h_data);\n",
        "  checkCudaErrors( cudaFree(d_idata) );\n",
        "  checkCudaErrors( cudaFree(d_odata) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "}\n"
      ],
      "metadata": {
        "id": "yOzhHOjmEV13",
        "outputId": "e96452dc-ba93-4d20-e058-39ede0cd13e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reduction.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc reduction.cu -o reduction -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83fd1fe5-5fd0-4cb0-e04b-1c3806ba6fe2",
        "id": "GQ-jNWTbQKnG"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z9reductionPfS_' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z9reductionPfS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 10 registers, 368 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "806f78b7-618a-4c73-9b6b-133f33fa4951",
        "id": "FBtDlLTRQKnJ"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "reduction error = 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7\n",
        "\n",
        "Modify the block-level reduction to use shuffle instructions as described in\n",
        "Lecture 4. Again your notebook should include your code, and results to show\n",
        "that the calculation has been carried out successfully."
      ],
      "metadata": {
        "id": "zXSco4UpRG5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reduction.cu\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "//\n",
        "// Practical 4 -- initial code for shared memory reduction for\n",
        "//                a single block which is a power of two in size\n",
        "//\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "#include <float.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "float reduction_gold(float* idata, int len)\n",
        "{\n",
        "  float sum = 0.0f;\n",
        "  for(int i=0; i<len; i++) sum += idata[i];\n",
        "\n",
        "  return sum;\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void reduction(float *g_odata, float *g_idata)\n",
        "{\n",
        "    extern  __shared__  float temp[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    // first, each thread loads data into shared memory\n",
        "    temp[tid] = g_idata[tid + blockDim.x * blockIdx.x];\n",
        "\n",
        "    int warpSize = 32;\n",
        "    int laneId = tid % warpSize;\n",
        "    int warpId = tid / warpSize;\n",
        "\n",
        "    // warp reduction\n",
        "    for (int i = warpSize/2; i > 0; i = i/2)\n",
        "        temp[tid] += __shfl_down_sync(-1, temp[tid], i);\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // store warp results at the end\n",
        "    if (laneId == 0)\n",
        "        temp[blockDim.x - (blockDim.x/warpSize) + warpId] = temp[tid];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // final reduction by first warp only\n",
        "    if (tid < warpSize)\n",
        "    {\n",
        "        int numWarps = (blockDim.x + warpSize - 1) / warpSize;\n",
        "        float val = (tid < numWarps) ? temp[blockDim.x - numWarps + tid] : 0.0f;\n",
        "\n",
        "        for (int offset = warpSize / 2; offset > 0; offset /= 2)\n",
        "            val += __shfl_down_sync(-1, val, offset);\n",
        "\n",
        "        if (tid == 0)\n",
        "            atomicAdd(g_odata, val);\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Program main\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main( int argc, const char** argv)\n",
        "{\n",
        "  int num_blocks, num_threads, num_elements, mem_size, shared_mem_size;\n",
        "\n",
        "  float *h_data, *d_idata, *d_odata;\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  num_blocks   = 1000;  // start with only 1 thread block\n",
        "  num_threads  = 512;\n",
        "  num_elements = num_blocks*num_threads;\n",
        "  mem_size     = sizeof(float) * num_elements;\n",
        "\n",
        "  // allocate host memory to store the input data\n",
        "  // and initialize to integer values between 0 and 10\n",
        "\n",
        "  h_data = (float*) malloc(mem_size);\n",
        "\n",
        "  for(int i = 0; i < num_elements; i++)\n",
        "    h_data[i] = floorf(10.0f*(rand()/(float)RAND_MAX));\n",
        "\n",
        "  // compute reference solution\n",
        "\n",
        "  float sum = reduction_gold(h_data, num_elements);\n",
        "\n",
        "  // allocate device memory input and output arrays\n",
        "\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_idata, mem_size) );\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_odata, sizeof(float)) );\n",
        "  checkCudaErrors( cudaMemset(d_odata, 0, sizeof(float)) );\n",
        "\n",
        "  // copy host memory to device input array\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(d_idata, h_data, mem_size,\n",
        "                              cudaMemcpyHostToDevice) );\n",
        "\n",
        "  // execute the kernel\n",
        "\n",
        "  shared_mem_size = sizeof(float) * num_threads;\n",
        "  reduction<<<num_blocks,num_threads,shared_mem_size>>>(d_odata,d_idata);\n",
        "  getLastCudaError(\"reduction kernel execution failed\");\n",
        "\n",
        "  // copy result from device to host\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(h_data, d_odata, sizeof(float),\n",
        "                              cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // check results\n",
        "\n",
        "  printf(\"reduction error = %f\\n\",h_data[0]-sum);\n",
        "\n",
        "  // cleanup memory\n",
        "\n",
        "  free(h_data);\n",
        "  checkCudaErrors( cudaFree(d_idata) );\n",
        "  checkCudaErrors( cudaFree(d_odata) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "}\n"
      ],
      "metadata": {
        "id": "cgMZHtNzRIu3",
        "outputId": "7b810907-5c1b-4908-9fc8-a36eca8782dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reduction.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc reduction.cu -o reduction -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64052e19-4e51-4140-ef22-a02f68748c8b",
        "id": "Bng7ODKdQMU1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mreduction.cu(48)\u001b[0m: \u001b[01;35mwarning\u001b[0m #68-D: integer conversion resulted in a change of sign\n",
            "          temp[tid] += __shfl_down_sync(-1, temp[tid], i);\n",
            "                                        ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mreduction.cu(65)\u001b[0m: \u001b[01;35mwarning\u001b[0m #68-D: integer conversion resulted in a change of sign\n",
            "              val += __shfl_down_sync(-1, val, offset);\n",
            "                                      ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mreduction.cu(48)\u001b[0m: \u001b[01;35mwarning\u001b[0m #68-D: integer conversion resulted in a change of sign\n",
            "          temp[tid] += __shfl_down_sync(-1, temp[tid], i);\n",
            "                                        ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mreduction.cu(65)\u001b[0m: \u001b[01;35mwarning\u001b[0m #68-D: integer conversion resulted in a change of sign\n",
            "              val += __shfl_down_sync(-1, val, offset);\n",
            "                                      ^\n",
            "\n",
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z9reductionPfS_' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z9reductionPfS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 12 registers, 368 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd7332e6-995e-42b2-98b8-74f826478212",
        "id": "AMRH9dNjQMU3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "reduction error = 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "tLhEaicpji2L"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "anQHnQHDTk-p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}